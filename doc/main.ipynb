{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "import pandas as pd\n",
    "# import re\n",
    "# from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Path:\n",
    "IMG_PATH = \"../data/pets/train/\"\n",
    "LABEL_PATH = \"../data/pets/train_label.txt\"\n",
    "TEST_IMG_PATH = \"../data/pets/test_practice/\"\n",
    "\n",
    "# Constants\n",
    "NUM_IMG = 2000\n",
    "N_CLUSTER = 20 # K-means \n",
    "NUM_TEST_IMG = 1850\n",
    "\n",
    "# Controls\n",
    "RUN_TEST = True\n",
    "RUN_TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Processing & General Feature Extraction\n",
    "\n",
    "# Gray scale image\n",
    "def gray(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT feature\n",
    "def get_features(image, SIFT_obj):\n",
    "    keypoints, features = SIFT_obj.detectAndCompute(image, None) # Don't need grayscale image here\n",
    "    return keypoints, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper Feature Extraction\n",
    "\n",
    "# Bag of word\n",
    "# clustering\n",
    "def bow_cluster(kmeans_obj, descriptor_stack):\n",
    "    kmeans_ret = kmeans_obj.fit_predict(descriptor_stack)\n",
    "    return kmeans_ret\n",
    "\n",
    "# generate vertical stack of descriptors\n",
    "def bow_vstack(desc_list):\n",
    "    stack = np.array(desc_list[0])\n",
    "    for rest in desc_list[1:]:\n",
    "        stack = np.vstack((stack, rest))\n",
    "    desc_stack = stack.copy()\n",
    "    return desc_stack\n",
    "\n",
    "# generate bag of words frequency matrix (shape: NUM_TRAIN_IMG * N_CLUSTER)\n",
    "def bow_get_freq_matrix(num_imgs, num_clusters, SIFT_list, kmeans_ret):\n",
    "    # initialization\n",
    "    matrix = np.array([np.zeros(num_clusters) for i in range(num_imgs)])\n",
    "    \n",
    "    # keep track of index of kmeans_ret\n",
    "    kmeans_id = 0\n",
    "    for i in range(num_imgs):\n",
    "        l = len(SIFT_list[i])\n",
    "        for j in range(l):\n",
    "            cluster_id = kmeans_ret[kmeans_id + j]\n",
    "            matrix[i][cluster_id] += 1\n",
    "        kmeans_id += l\n",
    "    \n",
    "    return matrix\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read In Image Files\n",
    "def get_images(path, num_img):\n",
    "    imlist = []\n",
    "    for i in range(num_img):\n",
    "        path_str = path + \"pet\" + str(i+1) + \".jpg\"\n",
    "        im = cv2.imread(path_str)\n",
    "        imlist.append(im)\n",
    "        \n",
    "    return imlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Images @ 2018-03-04 23:48:05.060112\n",
      "Complete @ 2018-03-04 23:48:14.088560 Time Cost: 0:00:09.028448\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"Reading Images @ \" + str(start_time))\n",
    "imlist = []\n",
    "if RUN_TRAIN:\n",
    "    imlist = get_images(IMG_PATH, NUM_IMG) # imlist contains a list of image numpy arrays\n",
    "elif RUN_TEST:\n",
    "    imlist = get_images(TEST_IMG_PATH, NUM_TEST_IMG)\n",
    "end_time = datetime.now()\n",
    "print(\"Complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 1850\n"
     ]
    }
   ],
   "source": [
    "print(\"Image count: \" + str(len(imlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read In Labels\n",
    "def get_labels(path):\n",
    "    labels = np.loadtxt(path, dtype = 'str')\n",
    "    np.reshape(labels, (-1, 1))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labellist = []\n",
    "if RUN_TRAIN:\n",
    "    labellist = get_labels(LABEL_PATH)\n",
    "    print(labellist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SIFT features @ 2018-03-04 23:48:23.424259\n",
      "SIFT features complete @ 2018-03-04 23:49:09.481780 Time Cost: 0:00:46.057521\n"
     ]
    }
   ],
   "source": [
    "## Compute SIFT features\n",
    "start_time = datetime.now()\n",
    "print(\"Generating SIFT features @ \" + str(start_time))\n",
    "SIFT_obj = cv2.xfeatures2d.SIFT_create()\n",
    "SIFT_list = []\n",
    "for im in imlist:\n",
    "    keypoint, descriptor = get_features(im, SIFT_obj)\n",
    "    SIFT_list.append(descriptor)\n",
    "end_time = datetime.now()\n",
    "print(\"SIFT features complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a vertical stack of descriptors to perform clustering\n",
    "if RUN_TRAIN:\n",
    "    start_time = datetime.now()\n",
    "    print(\"Generating SIFT stacked matrix @ \" + str(start_time))\n",
    "    descriptor_stack = bow_vstack(SIFT_list)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(\"Matrix complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))\n",
    "    print(\"Shape of vstack matrix: \" + str(descriptor_stack.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW clustering\n",
    "if RUN_TRAIN:\n",
    "    start_time = datetime.now()\n",
    "    print(\"Start Clustering @ \" + str(start_time))\n",
    "\n",
    "    kmeans_obj = KMeans(n_clusters = N_CLUSTER)\n",
    "    kmeans_ret = bow_cluster(kmeans_obj, descriptor_stack)\n",
    "    \n",
    "    kmeans_filename = '../output/models/kmeans' + str(N_CLUSTER) + '.sav'\n",
    "    print(\"Saving kmeans model\")\n",
    "    joblib.dump(kmeans_obj, kmeans_filename)\n",
    "    print(\"Saving complete\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(\"Matrix complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))\n",
    "    print(\"Shape of kmeans ret: \" + str(kmeans_ret.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAIN:\n",
    "    start_time = datetime.now()\n",
    "    print(\"Generating BOW Vocabulary @ \" + str(start_time))\n",
    "\n",
    "    vocab_matrix = bow_get_freq_matrix(NUM_IMG, N_CLUSTER, SIFT_list, kmeans_ret)\n",
    "\n",
    "    #### IMPORTANT: FOR SVM, STANDARDIZE DATA BEFORE FEEDING INTO SVC()\n",
    "    # scale = StandardScaler().fit(vocab_matrix)\n",
    "    # vocab_matrix_std = scale.transform(vocab_matrix)\n",
    "\n",
    "    df = pd.DataFrame(data = vocab_matrix)\n",
    "    df.index.name = 'IMG_ID'\n",
    "    column_name_list = []\n",
    "    for i in range(N_CLUSTER):\n",
    "        column_name_list.append('CLUSTER_ID_' + str(i))\n",
    "    df.columns = column_name_list\n",
    "    df.to_csv('../output/additionals/BOWmatrix-' + str(N_CLUSTER) + '.csv', mode = 'a', index = True, sep = ',')\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(\"BOW Vocabulary complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))\n",
    "    print(\"Plot Histogram of entire dataset (x = vocabulary, y = count)\")\n",
    "    x_scalar = np.arange(N_CLUSTER)\n",
    "    y_scalar = np.array([abs(np.sum(vocab_matrix[:, h], dtype = np.int32)) for h in range(N_CLUSTER)])\n",
    "    plt.bar(x_scalar, y_scalar)\n",
    "    plt.xlabel(\"Vocabulary Index\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"BOW frequency histogram\")\n",
    "    plt.xticks(x_scalar + 0.4, x_scalar)\n",
    "    plt.plot()\n",
    "    plt.savefig(\"../figs/BOW\" + str(N_CLUSTER) + \".png\")\n",
    "    print(\"Vocabulary Matrix shape: \" + str(vocab_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAIN:\n",
    "    train_labels = np.asarray(labellist)\n",
    "    print(train_labels.shape)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(vocab_matrix, train_labels, test_size = 0.2, random_state =42)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TRAIN:\n",
    "    start_time = datetime.now()\n",
    "    print(\"Start training using GBM @ \" + str(start_time))\n",
    "    \n",
    "    params = {'n_estimators': 2000, 'max_depth': 9, 'subsample': 0.5,\n",
    "          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = np.asarray(clf.predict(X_test))\n",
    "    accuracy = (pred == y_test).mean()\n",
    "    \n",
    "    gbm_filename = '../output/models/gbmModel' + str(N_CLUSTER) + '.sav'\n",
    "    joblib.dump(clf, gbm_filename)\n",
    "    \n",
    "    print(\"Validation Accuracy: %.2f%%\" % (accuracy * 100))\n",
    "    end_time = datetime.now()\n",
    "    print(\"End training @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "RUN_TEST = True\n",
    "if RUN_TEST:\n",
    "    kmeans_filename = '../output/models/kmeans' + str(N_CLUSTER) + '.sav'\n",
    "    gbm_filename = '../output/models/gbmModel' + str(N_CLUSTER) + '.sav'\n",
    "    \n",
    "    model = joblib.load(kmeans_filename)\n",
    "    clf = joblib.load(gbm_filename)\n",
    "    pred_tmp = []\n",
    "    for i in range(len(SIFT_list)):\n",
    "        vocab = np.array( [[ 0 for j in range(N_CLUSTER)]])\n",
    "        test_ret = model.predict(SIFT_list[i])\n",
    "        for k in test_ret:\n",
    "            vocab[0][k] += 1\n",
    "        pred_test = clf.predict(vocab)\n",
    "        pred_tmp.append(pred_test)\n",
    "        \n",
    "    pred_filename = \"../output/predictions/baseline.sav\"\n",
    "    joblib.dump(pred_tmp, pred_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
