{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import re\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Path:\n",
    "IMG_PATH = \"../data/pets/train/\"\n",
    "LABEL_PATH = \"../data/pets/train_label.txt\"\n",
    "\n",
    "# Constants\n",
    "NUM_IMG = 2000\n",
    "N_CLUSTER = 200 # K-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Processing & General Feature Extraction\n",
    "\n",
    "# Gray scale image\n",
    "def gray(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT feature\n",
    "def get_features(image, SIFT_obj):\n",
    "    keypoints, features = SIFT_obj.detectAndCompute(image, None) # Don't need grayscale image here\n",
    "    return keypoints, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper Feature Extraction\n",
    "\n",
    "# Bag of word\n",
    "# clustering\n",
    "def bow_cluster(kmeans_obj, descriptor_stack):\n",
    "    kmeans_ret = kmeans_obj.fit_predict(descriptor_stack)\n",
    "    return kmeans_ret\n",
    "\n",
    "# generate vertical stack of descriptors\n",
    "def bow_vstack(desc_list):\n",
    "    stack = np.array(desc_list[0])\n",
    "    for rest in desc_list[1:]:\n",
    "        stack = np.vstack((stack, rest))\n",
    "    desc_stack = stack.copy()\n",
    "    return desc_stack\n",
    "\n",
    "# generate bag of words frequency matrix (shape: NUM_TRAIN_IMG * N_CLUSTER)\n",
    "def bow_get_freq_matrix(num_imgs, num_clusters, SIFT_list, kmeans_ret):\n",
    "    # initialization\n",
    "    matrix = np.array([np.zeros(num_clusters) for i in range(num_imgs)])\n",
    "    \n",
    "    # keep track of index of kmeans_ret\n",
    "    kmeans_id = 0\n",
    "    for i in range(num_imgs):\n",
    "        l = len(SIFT_list[i])\n",
    "        for j in range(l):\n",
    "            cluster_id = kmeans_ret[kmeans_id + j]\n",
    "            matrix[i][cluster_id] += 1\n",
    "        kmeans_id += l\n",
    "    \n",
    "    return matrix\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read In Image Files\n",
    "def get_images(path):\n",
    "    imlist = []\n",
    "    for i in range(NUM_IMG):\n",
    "        path_str = path + \"pet\" + str(i+1) + \".jpg\"\n",
    "        im = cv2.imread(path_str)\n",
    "        imlist.append(im)\n",
    "        \n",
    "    return imlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"Reading Images @ \" + str(start_time))\n",
    "imlist = get_images(IMG_PATH) # imlist contains a list of image numpy arrays\n",
    "end_time = datetime.now()\n",
    "print(\"Complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(imlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read In Labels\n",
    "def get_labels(path):\n",
    "    labels = np.loadtxt(path, dtype = 'str')\n",
    "    np.reshape(labels, (-1, 1))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labellist = get_labels(LABEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labellist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute SIFT features\n",
    "start_time = datetime.now()\n",
    "print(\"Generating SIFT features @ \" + str(start_time))\n",
    "SIFT_obj = cv2.xfeatures2d.SIFT_create()\n",
    "SIFT_list = []\n",
    "for im in imlist:\n",
    "    keypoint, descriptor = get_features(im, SIFT_obj)\n",
    "    SIFT_list.append(descriptor)\n",
    "end_time = datetime.now()\n",
    "print(\"SIFT features complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a vertical stack of descriptors to perform clustering\n",
    "start_time = datetime.now()\n",
    "print(\"Generating SIFT stacked matrix @ \" + str(start_time))\n",
    "descriptor_stack = bow_vstack(SIFT_list)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"Matrix complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))\n",
    "print(\"Shape of vstack matrix: \" + str(descriptor_stack.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW clustering\n",
    "start_time = datetime.now()\n",
    "print(\"Start Clustering @ \" + str(start_time))\n",
    "\n",
    "kmeans_obj = KMeans(n_clusters = N_CLUSTER)\n",
    "kmeans_ret = bow_cluster(kmeans_obj, descriptor_stack)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"Matrix complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))\n",
    "print(\"Shape of kmeans ret: \" + str(kmeans_ret.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"Generating BOW Vocabulary @ \" + str(start_time))\n",
    "\n",
    "vocab_matrix = bow_get_freq_matrix(NUM_IMG, N_CLUSTER, SIFT_list, kmeans_ret)\n",
    "\n",
    "#### IMPORTANT: FOR SVM, STANDARDIZE DATA BEFORE FEEDING INTO SVC()\n",
    "scale = StandardScaler().fit(vocab_matrix)\n",
    "vocab_matrix_std = scale.transform(vocab_matrix)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"BOW Vocabulary complete @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))\n",
    "print(\"Plot Histogram of entire dataset (x = vocabulary, y = count)\")\n",
    "x_scalar = np.arange(N_CLUSTER)\n",
    "y_scalar = np.array([abs(np.sum(vocab_matrix[:, h], dtype = np.int32)) for h in range(N_CLUSTER)])\n",
    "plt.bar(x_scalar, y_scalar)\n",
    "plt.xlabel(\"Vocabulary Index\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"BOW frequency histogram\")\n",
    "plt.xticks(x_scalar + 0.4, x_scalar)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.asarray(labellist)\n",
    "print(train_labels.shape)\n",
    "print(vocab_matrix_std.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vocab_matrix_std, train_labels, test_size = 0.2, random_state =42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"Start training using SVM @ \" + str(start_time))\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = np.asarray(clf.predict(X_test))\n",
    "accuracy = (pred == y_test).mean()\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100))\n",
    "end_time = datetime.now()\n",
    "print(\"End training @ \" + str(end_time) + \" Time Cost: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary feature matrix to output folder in csv format and npy format\n",
    "from pandas import DataFrame\n",
    "df = DataFrame(data = vocab_matrix)\n",
    "df.index.name = 'IMG_ID'\n",
    "column_name_list = []\n",
    "for i in range(N_CLUSTER):\n",
    "    column_name_list.append('CLUSTER_ID_' + str(i))\n",
    "df.columns = column_name_list\n",
    "df.to_csv('../output/BOWmatrix-' + str(N_CLUSTER) + '.csv', mode = 'a', index = True, sep = ',')\n",
    "np.save('../output/BOWmatrix-' + str(N_CLUSTER) + '.npy', df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = DataFrame(data = vocab_matrix_std)\n",
    "df2.index.name = 'IMG_ID'\n",
    "df2.columns = column_name_list\n",
    "df2.to_csv('../output/BOWmatrix_std-' + str(N_CLUSTER) + '.csv', mode = 'a', index = True, sep = ',')\n",
    "np.save('../output/BOWmatrix_std' + str(N_CLUSTER) + '.npy', df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "kmeans_filename = '../output/kmeans' + str(N_CLUSTER) + '.sav'\n",
    "print(\"Saving kmeans model\")\n",
    "joblib.dump(kmeans_obj, kmeans_filename)\n",
    "print(\"Saving complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving\n",
    "model = joblib.load(kmeans_filename)\n",
    "SIFT_obj_tmp = cv2.xfeatures2d.SIFT_create()\n",
    "kp, des = get_features(imlist[0], SIFT_obj_tmp)\n",
    "print(\"Shape of descriptor: \" + str(des.shape))\n",
    "vocab = np.array( [[ 0 for i in range(N_CLUSTER)]])\n",
    "test_ret = model.predict(des)\n",
    "print(\"Shape of ret: \" + str(test_ret.shape))\n",
    "for i in test_ret:\n",
    "    vocab[0][i] += 1\n",
    "vocab = scale.transform(vocab)\n",
    "pred_test = clf.predict(vocab)\n",
    "print(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
